{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# <center> Ensemble Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "## <center> Bagging and Boosting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### <center>Decision Trees are prone to high variance and overfitting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "#### <center> by themselves..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## <center> Bagging"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- <b> How: </b> Multiple models are trained on subsets of the feature space.\n",
    "- <b> Why: </b> Reduce variance and overfitting.\n",
    "- <b> Method: </b> Parallel.\n",
    "- <b> Predictions: </b> Average.\n",
    "- <b> Example: </b> Random Forest."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## <center> Random Forest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "### The method:\n",
    " - Build various Decision Trees using random subsets of the features.\n",
    " - Combine the  predictions across all Decision Trees aand take the average."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<center><img src=\"randomforest.jpeg\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## <center> Boosting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- <b> How: </b> Base model is trained and subsequent models attempt to compensate for errors of previous models.\n",
    "- <b> Why: </b> Reduce bias and increase accuracy.\n",
    "- <b> Method: </b> Sequential.\n",
    "- <b> Predictions: </b> Weighted majority.\n",
    "- <b> Example: </b> AdaBoost."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## <center> Ada Boost"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "### The method:\n",
    " - Build baseline models and select the best one.\n",
    " - Iteratively build a new model, modifying weights of misclassified samples from the previous model.\n",
    " - Models are weighted by their accuracy and are combined through majority vote."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<center><img src=\"adaboost.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## <center> Combining Bagging and Boosting Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "### <center> Voting Classifier\n",
    "<center> Combines the prediction of each classifier into a single final vote."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "### <center> Stacking Classifier\n",
    "<center> Trains a meta-classifier using the base classifier predictions as features."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## <center> Activity\n",
    "<b>Data:</b> Environmental features such as temperature and humidity are used to detect the presence of humans in a room. <br>\n",
    "<b>Dataset:</b> <i>occupancy_data.csv</i> <br>\n",
    "<b>Goal:</b> Explore classification methods in order to predict whether or not someone in present in a room."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "1) Load in, explore the data, and scale the data with StandardScaler."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "2) Perform a train-test split using random_state=4 and test_size=0.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "3) Train two base classifiers using Logistic Regression and Decision Trees and record their accuracy and F1 scores.<br>\n",
    "-    What feature seems to be the most important for these classifiers?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "3) Try a bagging model (Random Forest) and a boosting model (Ada Boost) and store accuracy and F1 scores in a dataframe with the LR and DT models.<br>\n",
    "-    How do the models differ in performance?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "4) Try out a Voting Classifier and a Stacking Classifier and and add their scores to the dataframe.<br>\n",
    "-    How do these ensemble methods compare?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5) Generate confusion matrices for each of the models. \n",
    "- What does this reveal? \n",
    "- In what scenarios would one model be better over the others?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "6) A recent reading from a room gave the following data:\n",
    "- Temperature: 24 degrees C\n",
    "- Humidity: 27.2050 g/m3\n",
    "- Light: 420.5 lux\n",
    "- CO2: 709.25 ppm\n",
    "- Humidity Ratio: 0.004668\n",
    "<br><br>\n",
    "Is there someone in the room?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
