{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# <center> Natural Language Processing (NLP) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### <center> Applications</center>\n",
    "<center>\n",
    "- Translation <br>\n",
    "- Sentiment analysis <br>\n",
    "- Question answering <br>\n",
    "- Chatbots \n",
    "   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### <center> Challenges of NLP\n",
    "\n",
    "Difficult to understand meaning for a computer:\n",
    "\n",
    "    I was led to believe that the Fyre Festival would be an amazing, transcendent event - I was conned.\n",
    "\n",
    "Ambiguity because of lack of context (meaning or semantics):\n",
    "\n",
    "    The pipe couldn't fit through the hole in the wall since it was too big.\n",
    "\n",
    "versus:\n",
    "\n",
    "    The pipe couldn't fit through the hole in the wall since it was too small.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## <center> NLTK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "import nltk"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### <center> Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['This', 'is', 'a', 'sentence', 'with', '7', 'words', '.']"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentence = \"This is a sentence with 7 words.\"\n",
    "nltk.word_tokenize(sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['This is a sentence.',\n",
       " 'This is another sentence.',\n",
       " 'This is a third sentence']"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "paragraph = \"This is a sentence. This is another sentence. This is a third sentence\"\n",
    "nltk.sent_tokenize(paragraph)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### <center> Remove punctuation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "sentence_stripped = sentence.translate(str.maketrans('', '', string.punctuation))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['This', 'is', 'a', 'sentence', 'with', '7', 'words']"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.word_tokenize(sentence_stripped)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### <center> Stop words</center>\n",
    "<center> Stop words = noise/common words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Matthew\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping corpora\\stopwords.zip.\n",
      "{'mightn', 'shan', \"weren't\", 'nor', 'my', 'how', 'yours', \"won't\", 'not', 'now', 'our', 'while', 'few', 'couldn', 'over', 'd', 'by', \"hadn't\", 'needn', 'if', 'does', 'on', 'ma', 'under', 'herself', 'off', 'her', 'it', 'both', 'only', 'them', 'because', 's', 'are', 'mustn', 'a', 'having', 'or', 'wouldn', 'to', 're', 'before', 'an', \"she's\", 'down', 'of', \"you've\", 'further', \"wouldn't\", 'this', 've', \"you'll\", 'from', 'and', 'your', 'is', 'he', 't', 'doing', \"mightn't\", 'shouldn', 'should', 'itself', 'be', 'ours', 'o', 'as', 'his', \"haven't\", 'any', 'but', \"it's\", 'hasn', 'at', 'y', \"you're\", 'more', 'in', 'up', 'am', 'has', 'into', 'myself', 'had', 'all', 'hadn', \"that'll\", 'some', 'with', 'each', 'such', 'about', 'until', \"shan't\", 'these', 'against', 'don', \"isn't\", 'will', 'haven', 'between', 'me', 'did', 'what', 'theirs', 'same', 'yourselves', 'doesn', 'so', 'won', 'themselves', 'other', \"should've\", 'himself', 'hers', \"mustn't\", 'ain', 'there', 'where', 'was', \"hasn't\", 'm', 'then', 'can', 'own', 'll', 'been', 'through', 'didn', \"didn't\", 'i', 'were', 'which', 'being', 'during', 'here', 'wasn', 'we', 'when', \"shouldn't\", 'for', 'who', 'their', \"you'd\", 'no', \"don't\", 'weren', 'whom', 'above', \"couldn't\", 'why', 'they', 'just', 'again', \"needn't\", 'the', 'too', 'that', \"aren't\", 'him', 'most', 'aren', \"wasn't\", 'below', 'have', 'very', 'you', 'after', 'ourselves', 'isn', 'once', 'yourself', \"doesn't\", 'out', 'those', 'than', 'do', 'its', 'she'}\n"
     ]
    }
   ],
   "source": [
    "nltk.download('stopwords')\n",
    "stop_words=set(nltk.corpus.stopwords.words(\"english\"))\n",
    "print(stop_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Hello', 'Mr', 'Smith', 'How', 'today']"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentence = \"Hello Mr. Smith. How are you doing today?\"\n",
    "sentence_stripped = sentence.translate(str.maketrans('', '', string.punctuation))\n",
    "tokens = nltk.word_tokenize(sentence_stripped)\n",
    "filtered_tokens = [x for x in tokens if x not in stop_words]\n",
    "filtered_tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### <center> Stemming </center>\n",
    "<center> Reducing words to their roots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "ps = nltk.stem.PorterStemmer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'run'"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ps.stem(\"running\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### <center> Lemmatization </center>\n",
    "<center> Reducing words to their base words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "lem = nltk.stem.wordnet.WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('fly', 'fli')"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lem.lemmatize(\"flying\", \"v\"), ps.stem(\"flying\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'good'"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lem.lemmatize(\"better\", \"a\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### <center> Part of Speech Tagging "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Albert', 'NNP'),\n",
       " ('Einstein', 'NNP'),\n",
       " ('was', 'VBD'),\n",
       " ('born', 'VBN'),\n",
       " ('in', 'IN'),\n",
       " ('Ulm', 'NNP'),\n",
       " (',', ','),\n",
       " ('Germany', 'NNP'),\n",
       " ('in', 'IN'),\n",
       " ('1879', 'CD'),\n",
       " ('.', '.')]"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sent = \"Albert Einstein was born in Ulm, Germany in 1879.\"\n",
    "tokens = nltk.word_tokenize(sent)\n",
    "nltk.pos_tag(tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### <center> TF-IDF </center>\n",
    "<center> Term Frequency-Inverse Document Frequency"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<center><img src=\"tfidf.png\" height=600 width=600>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "def tokenize(text):\n",
    "    tokens = nltk.word_tokenize(text)\n",
    "    stems = []\n",
    "    for item in tokens:\n",
    "        stems.append(nltk.stem.PorterStemmer().stem(item))\n",
    "    return stems"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "tf_idf_vec = TfidfVectorizer(tokenizer=tokenize, stop_words='english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Steinbeck\\Pearl1.txt\n",
      "Steinbeck\\Pearl2.txt\n",
      "Steinbeck\\Pearl3.txt\n",
      "Steinbeck\\Pearl4.txt\n",
      "Steinbeck\\Pearl5.txt\n",
      "Steinbeck\\Pearl6.txt\n"
     ]
    }
   ],
   "source": [
    "import os \n",
    "token_dict = {}\n",
    "for dirpath, dirs, files in os.walk('Steinbeck'):\n",
    "    for f in files:\n",
    "        fname = os.path.join(dirpath, f)\n",
    "        print(fname)\n",
    "        with open(fname) as pearl:\n",
    "            text = pearl.read()\n",
    "            token_dict[f] = text.lower().translate(str.maketrans('', '', string.punctuation))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Matthew\\Anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:385: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'afterward', 'alon', 'alreadi', 'alway', 'ani', 'anoth', 'anyon', 'anyth', 'anywher', 'becam', 'becaus', 'becom', 'befor', 'besid', 'cri', 'describ', 'dure', 'els', 'elsewher', 'empti', 'everi', 'everyon', 'everyth', 'everywher', 'fifti', 'formerli', 'forti', 'ha', 'henc', 'hereaft', 'herebi', 'hi', 'howev', 'hundr', 'inde', 'latterli', 'mani', 'meanwhil', 'moreov', 'mostli', 'nobodi', 'noon', 'noth', 'nowher', 'onc', 'onli', 'otherwis', 'ourselv', 'perhap', 'pleas', 'seriou', 'sever', 'sinc', 'sincer', 'sixti', 'someon', 'someth', 'sometim', 'somewher', 'themselv', 'thenc', 'thereaft', 'therebi', 'therefor', 'thi', 'thu', 'togeth', 'twelv', 'twenti', 'veri', 'wa', 'whatev', 'whenc', 'whenev', 'wherea', 'whereaft', 'wherebi', 'wherev', 'whi', 'yourselv'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<6x2307 sparse matrix of type '<class 'numpy.float64'>'\n",
       "\twith 4617 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectors = tf_idf_vec.fit_transform(token_dict.values())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (0, 2024)\t0.3759573580043784\n",
      "  (0, 1143)\t0.8469428138456193\n",
      "  (0, 851)\t0.3759573580043784\n"
     ]
    }
   ],
   "source": [
    "phrase = 'all great and precious things are lonely.'\n",
    "response = tf_idf_vec.transform([phrase])\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'lone'"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf_idf_vec.get_feature_names()[1143]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### <center> Artist Lyric Classification Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>song</th>\n",
       "      <th>year</th>\n",
       "      <th>artist</th>\n",
       "      <th>genre</th>\n",
       "      <th>lyrics</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>ego-remix</td>\n",
       "      <td>2009</td>\n",
       "      <td>beyonce-knowles</td>\n",
       "      <td>Pop</td>\n",
       "      <td>Oh baby, how you doing?\\nYou know I'm gonna cu...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>then-tell-me</td>\n",
       "      <td>2009</td>\n",
       "      <td>beyonce-knowles</td>\n",
       "      <td>Pop</td>\n",
       "      <td>playin' everything so easy,\\nit's like you see...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>honesty</td>\n",
       "      <td>2009</td>\n",
       "      <td>beyonce-knowles</td>\n",
       "      <td>Pop</td>\n",
       "      <td>If you search\\nFor tenderness\\nIt isn't hard t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>you-are-my-rock</td>\n",
       "      <td>2009</td>\n",
       "      <td>beyonce-knowles</td>\n",
       "      <td>Pop</td>\n",
       "      <td>Oh oh oh I, oh oh oh I\\n[Verse 1:]\\nIf I wrote...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>black-culture</td>\n",
       "      <td>2009</td>\n",
       "      <td>beyonce-knowles</td>\n",
       "      <td>Pop</td>\n",
       "      <td>Party the people, the people the party it's po...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   index             song  year           artist genre  \\\n",
       "0      0        ego-remix  2009  beyonce-knowles   Pop   \n",
       "1      1     then-tell-me  2009  beyonce-knowles   Pop   \n",
       "2      2          honesty  2009  beyonce-knowles   Pop   \n",
       "3      3  you-are-my-rock  2009  beyonce-knowles   Pop   \n",
       "4      4    black-culture  2009  beyonce-knowles   Pop   \n",
       "\n",
       "                                              lyrics  \n",
       "0  Oh baby, how you doing?\\nYou know I'm gonna cu...  \n",
       "1  playin' everything so easy,\\nit's like you see...  \n",
       "2  If you search\\nFor tenderness\\nIt isn't hard t...  \n",
       "3  Oh oh oh I, oh oh oh I\\n[Verse 1:]\\nIf I wrote...  \n",
       "4  Party the people, the people the party it's po...  "
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "df = pd.read_csv('lyrics.csv')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "df.dropna(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "249"
      ]
     },
     "execution_count": 129,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(df[df['artist']=='beyonce-knowles'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "beyonce = df[df['artist']=='beyonce-knowles'].sample(30).index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "barbara = df[df['artist']=='barbra-streisand'].sample(30).index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "data = df[df.index.isin(beyonce) | df.index.isin(barbara)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>artist</th>\n",
       "      <th>lyrics</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>If you search\\nFor tenderness\\nIt isn't hard t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0</td>\n",
       "      <td>[Verse 1:]\\nI read all of the magazines\\nwhile...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>0</td>\n",
       "      <td>You're bad for me I clearly get it\\nI don't se...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>0</td>\n",
       "      <td>Ay! Ay! Ay!\\nOh! Beyonc, Beyonc\\nOh! Beyonc, B...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>0</td>\n",
       "      <td>You don't care about me at all\\nYou treat me l...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    artist                                             lyrics\n",
       "2        0  If you search\\nFor tenderness\\nIt isn't hard t...\n",
       "8        0  [Verse 1:]\\nI read all of the magazines\\nwhile...\n",
       "16       0  You're bad for me I clearly get it\\nI don't se...\n",
       "24       0  Ay! Ay! Ay!\\nOh! Beyonc, Beyonc\\nOh! Beyonc, B...\n",
       "30       0  You don't care about me at all\\nYou treat me l..."
      ]
     },
     "execution_count": 217,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>artist</th>\n",
       "      <th>lyrics</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>beyonce-knowles</td>\n",
       "      <td>If you search\\nFor tenderness\\nIt isn't hard t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>beyonce-knowles</td>\n",
       "      <td>[Verse 1:]\\nI read all of the magazines\\nwhile...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>beyonce-knowles</td>\n",
       "      <td>You're bad for me I clearly get it\\nI don't se...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>beyonce-knowles</td>\n",
       "      <td>Ay! Ay! Ay!\\nOh! Beyonc, Beyonc\\nOh! Beyonc, B...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>beyonce-knowles</td>\n",
       "      <td>You don't care about me at all\\nYou treat me l...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             artist                                             lyrics\n",
       "2   beyonce-knowles  If you search\\nFor tenderness\\nIt isn't hard t...\n",
       "8   beyonce-knowles  [Verse 1:]\\nI read all of the magazines\\nwhile...\n",
       "16  beyonce-knowles  You're bad for me I clearly get it\\nI don't se...\n",
       "24  beyonce-knowles  Ay! Ay! Ay!\\nOh! Beyonc, Beyonc\\nOh! Beyonc, B...\n",
       "30  beyonce-knowles  You don't care about me at all\\nYou treat me l..."
      ]
     },
     "execution_count": 174,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = data.drop(['index','song','year','genre'], axis=1)\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>artist</th>\n",
       "      <th>lyrics</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>If you search\\nFor tenderness\\nIt isn't hard t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0</td>\n",
       "      <td>[Verse 1:]\\nI read all of the magazines\\nwhile...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>0</td>\n",
       "      <td>You're bad for me I clearly get it\\nI don't se...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>0</td>\n",
       "      <td>Ay! Ay! Ay!\\nOh! Beyonc, Beyonc\\nOh! Beyonc, B...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>0</td>\n",
       "      <td>You don't care about me at all\\nYou treat me l...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    artist                                             lyrics\n",
       "2        0  If you search\\nFor tenderness\\nIt isn't hard t...\n",
       "8        0  [Verse 1:]\\nI read all of the magazines\\nwhile...\n",
       "16       0  You're bad for me I clearly get it\\nI don't se...\n",
       "24       0  Ay! Ay! Ay!\\nOh! Beyonc, Beyonc\\nOh! Beyonc, B...\n",
       "30       0  You don't care about me at all\\nYou treat me l..."
      ]
     },
     "execution_count": 175,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['artist'] = [0 if x=='beyonce-knowles' else 1 for x in data['artist']]\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "## Count Vectorizer \n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "matrix = CountVectorizer(max_features=10000)\n",
    "X = matrix.fit_transform(data['lyrics']).toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1.0, 0.9166666666666666)"
      ]
     },
     "execution_count": 195,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, data['artist'], test_size=0.2, random_state=123)\n",
    "clf = RandomForestClassifier(random_state=123).fit(X_train, y_train)\n",
    "clf.score(X_train, y_train), clf.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Matthew\\Anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:385: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'afterward', 'alon', 'alreadi', 'alway', 'ani', 'anoth', 'anyon', 'anyth', 'anywher', 'becam', 'becaus', 'becom', 'befor', 'besid', 'cri', 'describ', 'dure', 'els', 'elsewher', 'empti', 'everi', 'everyon', 'everyth', 'everywher', 'fifti', 'formerli', 'forti', 'ha', 'henc', 'hereaft', 'herebi', 'hi', 'howev', 'hundr', 'inde', 'latterli', 'mani', 'meanwhil', 'moreov', 'mostli', 'nobodi', 'noon', 'noth', 'nowher', 'onc', 'onli', 'otherwis', 'ourselv', 'perhap', 'pleas', 'seriou', 'sever', 'sinc', 'sincer', 'sixti', 'someon', 'someth', 'sometim', 'somewher', 'themselv', 'thenc', 'thereaft', 'therebi', 'therefor', 'thi', 'thu', 'togeth', 'twelv', 'twenti', 'veri', 'wa', 'whatev', 'whenc', 'whenev', 'wherea', 'whereaft', 'wherebi', 'wherev', 'whi', 'yourselv'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n"
     ]
    }
   ],
   "source": [
    "## TF-IDF Vectorizer\n",
    "tf_idf_vec = TfidfVectorizer(tokenizer=tokenize, stop_words='english')\n",
    "vectors = tf_idf_vec.fit_transform(data['lyrics'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1.0, 0.8333333333333334)"
      ]
     },
     "execution_count": 197,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(vectors, data['artist'], test_size=0.2, random_state=123)\n",
    "clf = RandomForestClassifier(random_state=123).fit(X_train, y_train)\n",
    "clf.score(X_train, y_train), clf.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('If', 'you')\n",
      "('you', 'search')\n",
      "('search', 'For')\n",
      "('For', 'tenderness')\n",
      "('tenderness', 'It')\n",
      "('It', 'is')\n",
      "('is', \"n't\")\n",
      "(\"n't\", 'hard')\n",
      "('hard', 'to')\n",
      "('to', 'find')\n",
      "('find', 'You')\n",
      "('You', 'can')\n",
      "('can', 'have')\n",
      "('have', 'the')\n",
      "('the', 'love')\n",
      "('love', 'You')\n",
      "('You', 'need')\n",
      "('need', 'to')\n",
      "('to', 'live')\n",
      "('live', 'But')\n",
      "('But', 'if')\n",
      "('if', 'you')\n",
      "('you', 'look')\n",
      "('look', 'For')\n",
      "('For', 'truthfulness')\n",
      "('truthfulness', 'You')\n",
      "('You', 'might')\n",
      "('might', 'just')\n",
      "('just', 'As')\n",
      "('As', 'well')\n",
      "('well', 'be')\n",
      "('be', 'blind')\n",
      "('blind', 'It')\n",
      "('It', 'always')\n",
      "('always', 'seems')\n",
      "('seems', 'to')\n",
      "('to', 'be')\n",
      "('be', 'So')\n",
      "('So', 'hard')\n",
      "('hard', 'to')\n",
      "('to', 'give')\n",
      "('give', 'Chorus')\n",
      "('Chorus', ':')\n",
      "(':', 'Honesty')\n",
      "('Honesty', 'Is')\n",
      "('Is', 'such')\n",
      "('such', 'a')\n",
      "('a', 'lonely')\n",
      "('lonely', 'word')\n",
      "('word', 'Everyone')\n",
      "('Everyone', 'is')\n",
      "('is', 'so')\n",
      "('so', 'untrue')\n",
      "('untrue', 'Honesty')\n",
      "('Honesty', 'Is')\n",
      "('Is', 'hardly')\n",
      "('hardly', 'ever')\n",
      "('ever', 'heard')\n",
      "('heard', 'And')\n",
      "('And', 'mostly')\n",
      "('mostly', 'What')\n",
      "('What', 'I')\n",
      "('I', 'need')\n",
      "('need', 'from')\n",
      "('from', 'you')\n",
      "('you', 'I')\n",
      "('I', 'can')\n",
      "('can', 'always')\n",
      "('always', 'Find')\n",
      "('Find', 'someone')\n",
      "('someone', 'To')\n",
      "('To', 'say')\n",
      "('say', 'They')\n",
      "('They', 'sympathize')\n",
      "('sympathize', 'If')\n",
      "('If', 'I')\n",
      "('I', 'wear')\n",
      "('wear', 'my')\n",
      "('my', 'heart')\n",
      "('heart', 'Out')\n",
      "('Out', 'on')\n",
      "('on', 'my')\n",
      "('my', 'sleeve')\n",
      "('sleeve', 'But')\n",
      "('But', 'I')\n",
      "('I', 'do')\n",
      "('do', \"n't\")\n",
      "(\"n't\", 'want')\n",
      "('want', 'Some')\n",
      "('Some', 'pretty')\n",
      "('pretty', 'face')\n",
      "('face', 'To')\n",
      "('To', 'tell')\n",
      "('tell', 'me')\n",
      "('me', 'Pretty')\n",
      "('Pretty', 'lies')\n",
      "('lies', 'All')\n",
      "('All', 'I')\n",
      "('I', 'want')\n",
      "('want', 'Is')\n",
      "('Is', 'someone')\n",
      "('someone', 'To')\n",
      "('To', 'believe')\n",
      "('believe', '(')\n",
      "('(', 'Chorus')\n",
      "('Chorus', ')')\n",
      "(')', 'I')\n",
      "('I', 'can')\n",
      "('can', 'find')\n",
      "('find', 'a')\n",
      "('a', 'lover')\n",
      "('lover', 'I')\n",
      "('I', 'can')\n",
      "('can', 'find')\n",
      "('find', 'a')\n",
      "('a', 'friend')\n",
      "('friend', 'I')\n",
      "('I', 'can')\n",
      "('can', 'have')\n",
      "('have', 'security')\n",
      "('security', 'Until')\n",
      "('Until', 'the')\n",
      "('the', 'bitter')\n",
      "('bitter', 'end')\n",
      "('end', 'Anyone')\n",
      "('Anyone', 'can')\n",
      "('can', 'comfort')\n",
      "('comfort', 'me')\n",
      "('me', 'With')\n",
      "('With', 'promises')\n",
      "('promises', 'again')\n",
      "('again', 'I')\n",
      "('I', 'know')\n",
      "('know', ',')\n",
      "(',', 'I')\n",
      "('I', 'know')\n",
      "('know', 'When')\n",
      "('When', 'I')\n",
      "('I', \"'m\")\n",
      "(\"'m\", 'deep')\n",
      "('deep', 'Inside')\n",
      "('Inside', 'of')\n",
      "('of', 'me')\n",
      "('me', 'Do')\n",
      "('Do', \"n't\")\n",
      "(\"n't\", 'be')\n",
      "('be', 'Too')\n",
      "('Too', 'concerned')\n",
      "('concerned', 'I')\n",
      "('I', 'wo')\n",
      "('wo', \"n't\")\n",
      "(\"n't\", 'ask')\n",
      "('ask', 'For')\n",
      "('For', \"nothin'\")\n",
      "(\"nothin'\", 'While')\n",
      "('While', 'I')\n",
      "('I', \"'m\")\n",
      "(\"'m\", 'gone')\n",
      "('gone', 'But')\n",
      "('But', 'when')\n",
      "('when', 'I')\n",
      "('I', 'want')\n",
      "('want', 'Sincerity')\n",
      "('Sincerity', 'Tell')\n",
      "('Tell', 'me')\n",
      "('me', 'where')\n",
      "('where', 'else')\n",
      "('else', 'Can')\n",
      "('Can', 'I')\n",
      "('I', 'turn')\n",
      "('turn', 'When')\n",
      "('When', 'You')\n",
      "('You', \"'re\")\n",
      "(\"'re\", 'the')\n",
      "('the', 'one')\n",
      "('one', 'That')\n",
      "('That', 'I')\n",
      "('I', 'depend')\n",
      "('depend', 'upon')\n",
      "('upon', '(')\n",
      "('(', 'Chorus')\n",
      "('Chorus', ')')\n"
     ]
    }
   ],
   "source": [
    "## Bigrams\n",
    "for bigram in nltk.bigrams(nltk.word_tokenize(data['lyrics'].values[0])):\n",
    "    print(bigram)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1.0, 0.75)"
      ]
     },
     "execution_count": 222,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "matrix = CountVectorizer(max_features=10000, ngram_range=(2,2))\n",
    "X = matrix.fit_transform(data['lyrics']).toarray()\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, data['artist'], test_size=0.2, random_state=123)\n",
    "clf = RandomForestClassifier(random_state=123).fit(X_train, y_train)\n",
    "clf.score(X_train, y_train), clf.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## <center> Activity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<center> Use NLP and classification to perform a sentiment analysis task - predicting the sentiment of a Twitter user on nuclear power based off the text of their tweet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv('sentiment_nuclear_power.csv',encoding = 'unicode_escape')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
