{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <center>Web Scraping Into SQL Database Demo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src = 'anime.png'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The goal"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Scrape the site https://myanimelist.net/ in order to compile a SQL database of various anime data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this demo, we will grab the id, name, number of episodes, and a couple recommended shows for each of the top anime."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The process"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import time\n",
    "import random\n",
    "import math\n",
    "import sqlite3\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this demo, we will look just at some of the most popular anime found here https://myanimelist.net/topanime.php?type=bypopularity."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scraping links"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Make initial request to the URL and parse wth BS."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = 'https://myanimelist.net/topanime.php?type=bypopularity'\n",
    "response = requests.get(url)\n",
    "soup = BeautifulSoup(response.content, 'html.parser')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Find the links to the individual pages for each of the top anime shows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "urls = []\n",
    "for i in soup.find_all('div', class_=\"di-ib clearfix\"):\n",
    "    urls.append(i.find('a', href=True)['href'])\n",
    "urls[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scraping in a loop"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a function that will grab the id, name, and number of episodes for a given anime URL. Also, it will grab the top two recommendations, as well as the id and URL for those recommendations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_name_episodes(url, rec=False):\n",
    "    response = requests.get(url, headers=headers)\n",
    "    print(response.status_code, url)\n",
    "    soup = BeautifulSoup(response.content, 'html.parser')\n",
    "    main_id = url.split('/')[4]\n",
    "    episodes = soup.find('span', text='Episodes:').next_element.strip()\n",
    "    name = soup.find('h1').text.strip()\n",
    "    print(name)\n",
    "    rec_urls=[]\n",
    "    rec_ids=[]\n",
    "    if rec:\n",
    "        response = requests.get(url+'/userrecs')\n",
    "        soup = BeautifulSoup(response.content, 'html.parser')\n",
    "        recs = soup.find_all('div', style='margin-bottom: 2px;', limit=2)\n",
    "        for r in recs:\n",
    "            rec_url = r.find('a', href=True)['href']\n",
    "            rec_urls.append(rec_url)\n",
    "            rec_id = rec_url.split('/')[4]\n",
    "            rec_ids.append(rec_id)\n",
    "    return main_id, episodes, name, rec_urls, rec_ids"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to make our scraping activity look normal, we can pass in a header containing a 'User-agent' key. This basically tells the website we are just a normal web browser. <br>\n",
    "Also, we want to have short, random pauses in between requests. This will mimic human behaviour and help to prevent the risk of being flagged as a bot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "headers = {'User-agent': 'Mozilla/5.0 (Windows NT 6.1; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/64.0.3282.186 Safari/537.36'}\n",
    "sleep_min = 0\n",
    "sleep_max = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The main loop. Now we want to go through each of the URLs, run our scraping function, then run the scraping function on the recommendations as well. <br> <br>\n",
    "We want to store all the information we gather in lists along the way. <br> <br>\n",
    "Some of the recommendations may be shows we already scraped. By using a set() of ids we have already scraped, we can avoid scraping the same show page multiple times."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "now = time.time()\n",
    "episode_counts = []\n",
    "names = []\n",
    "ids = []\n",
    "ids_set = set()\n",
    "recs_main = []\n",
    "recs1 = []\n",
    "recs2 = []\n",
    "sleep_min = 0\n",
    "sleep_max = 1\n",
    "for url in urls[:5]:\n",
    "    main_id, episodes, name, rec_urls, rec_ids = get_name_episodes(url, True)\n",
    "    \n",
    "    if main_id not in ids_set:\n",
    "        ids.append(main_id)\n",
    "        episode_counts.append(episodes)\n",
    "        names.append(name)\n",
    "        recs_main.append(main_id)\n",
    "        recs1.append(rec_ids[0])\n",
    "        recs2.append(rec_ids[1])\n",
    "    \n",
    "    for u,i in zip(rec_urls, rec_ids):\n",
    "        if i not in ids_set:\n",
    "            time.sleep(random.uniform(sleep_min, sleep_max))\n",
    "            rec_id, episodes, name, _, _ = get_name_episodes(u)\n",
    "            \n",
    "            ids.append(rec_id)\n",
    "            episode_counts.append(episodes)\n",
    "            names.append(name)\n",
    "    time.sleep(random.uniform(sleep_min, sleep_max))\n",
    "print('Took', time.time()-now, 'seconds.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading into SQL database"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have the data, we can set up a SQL database."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initialize and connect to the database."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conn = sqlite3.connect('anime.db')\n",
    "cur = conn.cursor()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create anime table containing the id, name, and number of episodes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cur.execute('''\n",
    "            CREATE TABLE anime(\n",
    "            id INT PRIMARY KEY,\n",
    "            name TEXT,\n",
    "            episodes INT\n",
    "            )\n",
    "            ''')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Insert the anime data into the anime table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i,name,eps in zip(ids, names, episode_counts):\n",
    "    cur.execute('''\n",
    "                INSERT OR REPLACE INTO anime\n",
    "                VALUES(?, ?, ?)\n",
    "                ''',(i,name,eps))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Inspect the anime table by loading it into a DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cur.execute('''\n",
    "            SELECT * FROM anime''')\n",
    "x = cur.fetchall()\n",
    "anime_df = pd.DataFrame(x)\n",
    "anime_df.columns = [i[0] for i in cur.description]\n",
    "anime_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create recommendations table containing the main show id, the first recommendation id, and the second recommendation id."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cur.execute('''\n",
    "            CREATE TABLE recs(\n",
    "            id INT,\n",
    "            first_rec_id INT,\n",
    "            second_rec_id INT\n",
    "            )\n",
    "            ''')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Insert the recommendation data into the recommendations table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i,r1,r2 in zip(recs_main,recs1,recs2):\n",
    "    cur.execute('''\n",
    "                INSERT INTO recs\n",
    "                VALUES(?, ?, ?)\n",
    "                ''',(i,r1,r2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Inspect the recommendations table by loading it into a DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cur.execute('''\n",
    "            SELECT * FROM recs''')\n",
    "x = cur.fetchall()\n",
    "recs_df = pd.DataFrame(x)\n",
    "recs_df.columns = [i[0] for i in cur.description]\n",
    "recs_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Commit the changes to the database."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conn.commit()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SQL command practice"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a DataFrame containing each of the most popular shows and the two recommendations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cur.execute('''\n",
    "            SELECT a1.name as \"if you like\", a2.name as \"try\", a3.name as \"or\" FROM anime a1\n",
    "            JOIN recs\n",
    "            ON a1.id=recs.id\n",
    "            JOIN anime a2\n",
    "            ON recs.first_rec_id=a2.id\n",
    "            JOIN anime a3\n",
    "            ON recs.second_rec_id=a3.id\n",
    "            GROUP BY a1.name\n",
    "            ''')\n",
    "x= cur.fetchall()\n",
    "all_df = pd.DataFrame(x)\n",
    "all_df.columns = [i[0] for i in cur.description]\n",
    "all_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
