{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# <center>Calculus and Gradient Descent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Outcomes\n",
    "<br>\n",
    "    - Calculate derivatives for any given function <br>\n",
    "    - Understand partial derivatives <br>\n",
    "    - Understand and implement gradient descent to perform linear regression <br>\n",
    "    - Gain more experience manipulating Numpy arrays"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Derivatives"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "x = np.linspace(-5,5)\n",
    "y = [i**2 for i in x]\n",
    "plt.plot(x,y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Power rule"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Constant factor rule"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Addition rule"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Chain rule"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Partial Derivatives"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Knowledge Check"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "\\begin{align}\n",
    "& f(x) =2/3x^9 \\\\ \\\\\n",
    "& f(x) =4x^4 - 5/3x^3 + 2x^2 - 5x + 7 \\\\ \\\\\n",
    "& f(x) = (3x^2 + 4x + 2)^3 \\\\ \\\\\n",
    "& f(x) = 5x^3 + 4y^2 + 3x + 2y + 8 \\\\ \\\\\n",
    "\\end{align}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Using derivatives to find minima and maxima"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### <center>Cost function<br>\n",
    "#### <center> A measure of error that needs to be minimized"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "<center> <img src='mse.jpg'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Gradient descent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><img src='grad_desc.png'>\n",
    "<center><img src='gradient_desc.gif' height=\"500\" width=\"500\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Step size and Learning Rate <br>\n",
    "Step size will be equal to the product of the current value of the derivative of the cost function and a learning rate."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "<img src='learning_rate.png'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Gradient Descent in Action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "X = np.array([1,2,3])\n",
    "y = np.array([2,3,7])\n",
    "plt.scatter(X,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_error \n",
    "m = 0\n",
    "b = 0\n",
    "lr = 0.1\n",
    "N = len(x)\n",
    "\n",
    "y_pred = X*m+b  ## [0, 0, 0]\n",
    "y_diff = y-y_pred ## [2, 3, 7]\n",
    "D_m = (-2/N)*sum(X*y_diff) ## [(-2/3)*(1*2 + 2*3 + 3*7)]\n",
    "D_b = (-2/N)*sum(y_diff) ## [(-2/3)*(2 + 3 + 7)]\n",
    "m = m - lr*D_m\n",
    "b = b - lr*D_b\n",
    "print('Slope: ',m)\n",
    "print('Intercept: ', b)\n",
    "print('MSE: ',mean_squared_error(y, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "m = 0\n",
    "b = 0\n",
    "lr = 0.1\n",
    "N = len(x)\n",
    "epochs = 100\n",
    "\n",
    "for i in range(epochs):\n",
    "    y_pred = X*m+b\n",
    "    y_diff = y-y_pred\n",
    "    D_m = (-2/N)*sum(X*y_diff)\n",
    "    D_b = (-2/N)*sum(y_diff)\n",
    "    m = m - lr*D_m\n",
    "    b = b - lr*D_b\n",
    "print('Slope: ',m)\n",
    "print('Intercept: ', b)\n",
    "print('MSE: ',mean_squared_error(y, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "plt.scatter(X,y)\n",
    "plt.plot(X,y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "def gradient_descent(X,y,lr,epochs):\n",
    "    N = len(X)\n",
    "    m = 0\n",
    "    b = 0\n",
    "    for i in range(epochs):\n",
    "        y_pred = X*m+b\n",
    "        y_diff = y-y_pred\n",
    "        D_m = (-2/N)*sum(X*y_diff)\n",
    "        D_b = (-2/N)*sum(y_diff)\n",
    "        m = m - lr*D_m\n",
    "        b = b - lr*D_b\n",
    "    return m,b,y_pred"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Another example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_csv('graduate_admission_data.csv')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "X = np.array(df['GRE Score'])\n",
    "y = np.array(df['Chance of Admit '])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "m,b,y_pred = gradient_descent(X,y,0.1,100)\n",
    "print('Slope: ',m)\n",
    "print('Intercept: ', b)\n",
    "print('MSE: ',mean_squared_error(y_pred,y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "scaler = MinMaxScaler()\n",
    "df[df.columns] = scaler.fit_transform(df[df.columns])\n",
    "X = np.array(df['GRE Score'])\n",
    "y = np.array(df['Chance of Admit '])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "m,b,y_pred = gradient_descent(X,y,0.1,2000)\n",
    "print('Slope: ',m)\n",
    "print('Intercept: ', b)\n",
    "print('MSE: ',mean_squared_error(y_pred,y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "import statsmodels.api as sm\n",
    "result = sm.OLS(y, sm.add_constant(X)).fit()\n",
    "print('MSE: ',result.mse_resid)\n",
    "result.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Activity\n",
    "Using the function we created for linear regression using gradient descent, implement a function that can perform multiple linear regression. <br><br>\n",
    "Instead of taking in one feature (x), the function should be able to handle any number of features (x1, x2, x3...xi)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "To do this, you will need to consider the following:\n",
    "- <b>m</b> will now be an array instead of a scalar value corresponding to the slope for each of the features\n",
    "- <b>y_pred</b> can be found by using matrix multiplication of the <b>X</b> matrix and the <b>m</b> array.\n",
    "- Similar to the calculation of <b>y_pred</b>, to calculate the derivatives with respect to each <b>m</b>, you can use matrix multiplication. <br><b>Note:</b> Remember how the shapes of matrices have to align in order to multiply them and how you can utilize a matrix transformation to get a matrix into the right shape."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "After completing your multiple gradient descent function, run it on all the features of the Graduate Admissions dataset. <br> <br>\n",
    "Feel free to experiment with different values for the learning rate and number of epochs. <br> <br>\n",
    "Then, compare your results to an OLS regression. <br> <br>\n",
    "What seems to be the best combination of learning rate and number of epochs for this data?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def multiple_gradient_descent(X,y,lr,epochs):\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
